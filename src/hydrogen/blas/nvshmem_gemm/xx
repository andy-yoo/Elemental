template<typename T>
void mcmr_to_vcstar(MPI_Comm, int, int, int, int, T*, T*, cudaStream_t)
{
  throw std::runtime_error("Function not implemented\n");
}

void mcmr_to_vcstar(MPI_Comm mpi_comm,
	int m,
	int n,
	int grid_height,
	int grid_width,
	float* dev_Matrix_buffer,
	float* dev_target_buffer,
	cudaStream_t stream)
{
	int grid_size = grid_height*grid_width;
        int my_pe_rank;
        std::vector<int> pes;
        int xnpes;
 	setup_pes(mpi_comm, &my_pe_rank, pes, &xnpes);

	int mcmr_local_height  = local_height_mcmr(my_pe_rank, m, n, grid_height, grid_width);
	int mcmr_local_width   = local_width_mcmr(my_pe_rank, m, n, grid_height, grid_width);

    	std::vector<int> send_counts(grid_size, 0);
    	std::vector<int> send_displs(grid_size+1, 0);
    	int total_send;
    	int max_send;
    	std::vector<int> recv_counts(grid_size, 0);
    	std::vector<int> recv_displs(grid_size+1, 0);
    	int total_recv;
    	int max_recv;

    	// Compute the maximum size for send and recv operation. Need this because
    	// these buffers are SDOs of nvshmem (OpenSHMEM constraint)
    	max_send = (int) (ceil(((double)m)/grid_height)*ceil(((double)n)/grid_width));
    	max_recv = (int) (ceil(((double)m)/grid_size)*n);

    	int num_threads;

    	std::vector<int> offset_counts (grid_size*2, 0);

    	counts_mcmr_to_vc_star(grid_height, grid_width, my_pe_rank, m, n,
                send_counts, send_displs, &total_send,
                recv_counts, recv_displs, &total_recv,
                offset_counts);
    	num_threads = min(total_send, 1024);

    	int* dev_send_displs = (int*) nvshmem_malloc(send_displs.size()*sizeof(int));
    	CHECK_CUDA(cudaMemcpy((void*) dev_send_displs, (void const*) send_displs.data(), send_displs.size()*sizeof(int), cudaMemcpyHostToDevice));

    	float* dev_send_buf = (float*) nvshmem_malloc(max_send*sizeof(float));
    	int* dev_target_offset_counts = (int*) nvshmem_malloc(offset_counts.size()*sizeof(int));
    	CHECK_CUDA(cudaMemcpy((void*) dev_target_offset_counts, (void const*) offset_counts.data(), offset_counts.size()*sizeof(int), cudaMemcpyHostToDevice));

    	float* dev_recv_buf = (float*) nvshmem_malloc(max_recv*sizeof(float));

        int dev_local_buffer_size = max(max_recv, mcmr_local_height*mcmr_local_width);
	dev_local_buffer_size = max(dev_local_buffer_size, max_send);
        float* dev_local_buffer = (float*) nvshmem_malloc(dev_local_buffer_size*sizeof(float));
        CHECK_CUDA(cudaMemcpy((void*) dev_local_buffer, (void const*) dev_Matrix_buffer, 
                mcmr_local_height*mcmr_local_width*sizeof(float), cudaMemcpyDeviceToDevice));

    	pack_mcmr_to_vc_star<<<1,num_threads,0, stream>>> (m, n, my_pe_rank, grid_height, grid_width, dev_send_displs, dev_local_buffer, dev_send_buf);

        int workspace_size = grid_size;
        int* common_workspace = (int*) nvshmem_malloc(workspace_size *sizeof(int));
        CHECK_CUDA(cudaMemset(common_workspace, 0, workspace_size*sizeof(int)));

        int* dev_pes = (int*) nvshmem_malloc(xnpes*sizeof(int));
        CHECK_CUDA(cudaMemcpy((void*) dev_pes, (void const*) pes.data(), xnpes*sizeof(int), cudaMemcpyHostToDevice));

    	int sync_counter=1;
    	Alltoallv_put(total_recv, my_pe_rank, dev_send_buf, dev_recv_buf, dev_send_displs, dev_target_offset_counts, dev_pes, xnpes, sync_counter, common_workspace, stream);

    	int* dev_recv_displs = (int*) nvshmem_malloc(grid_size+1);
    	CHECK_CUDA(cudaMemcpy((void*) dev_recv_displs, (void const*) recv_displs.data(), (grid_size+1)*sizeof(int), cudaMemcpyHostToDevice));

    	num_threads = min(max_recv, 1024);
    	unpack_mcmr_to_vc_star<<<1,num_threads,0,stream>>> (m, n, my_pe_rank, grid_height, grid_width, dev_recv_displs, dev_local_buffer, dev_recv_buf);

char line[132];
FILE* fp_debug;
sprintf(line, "___vcstar.%04d", my_pe_rank);
fp_debug = fopen(line, "w");

std::vector<float> B1_VC_STAR_mem_buffer(max_recv);
cudaMemcpy(B1_VC_STAR_mem_buffer.data(), dev_local_buffer, max_recv*sizeof(float), cudaMemcpyDeviceToHost);
fprintf(fp_debug, "Buffer of B_VC_STAR...\n");
for(int l=0; l<total_recv; l++){
       fprintf(fp_debug, "%f ", B1_VC_STAR_mem_buffer[l]);
}
fprintf(fp_debug, "\n");
fclose(fp_debug);

/*
sprintf(line, "memsizes.%04d", my_pe_rank);
fp_debug = fopen(line, "w");

fprintf(fp_debug, "dev_send_displs: %d\n", send_displs.size());
fprintf(fp_debug, "dev_send_buf:    %d\n", max_send);
fprintf(fp_debug, "dev_target_offset_counts: %d\n", offset_counts.size());
fprintf(fp_debug, "dev_recv_buf: %d\n", max_recv);
fprintf(fp_debug, "dev_local_buffer: %d\n", dev_local_buffer_size);
fprintf(fp_debug, "common_workspace: %d\n", workspace_size);
fprintf(fp_debug, "dev_pes: %d\n", xnpes);
fprintf(fp_debug, "dev_recv_displs: %d\n", grid_size+1);
fclose(fp_debug);
*/

	cudaMemcpy(dev_target_buffer, dev_local_buffer, local_recv_size_vcstar_to_vrstar*sizeof(float), cudaMemcpyDeviceToDevice);

	MPI_Barrier(mpi_comm);

        //sync_counter++;
 	//Global_sync (my_pe_rank, dev_pes, xnpes, sync_counter, common_workspace, 0);
	nvshmem_free(dev_recv_displs);
	nvshmem_free(dev_send_displs);
	nvshmem_free(dev_send_buf);
	nvshmem_free(dev_target_offset_counts);
	nvshmem_free(dev_recv_buf);
	nvshmem_free(dev_local_buffer);
	nvshmem_free(common_workspace);
	nvshmem_free(dev_pes);
}

template void mcmr_to_vcstar<int>(MPI_Comm, int, int, int, int, int*, int*, cudaStream_t);
template void mcmr_to_vcstar<double>(MPI_Comm, int, int, int, int, double*, double*, cudaStream_t);
template void mcmr_to_vcstar<__half>(MPI_Comm, int, int, int, int, __half*, __half*, cudaStream_t);
template void mcmr_to_vcstar<El::Complex<float>>(MPI_Comm, int, int, int, int, El::Complex<float>*, El::Complex<float>*, cudaStream_t);
template void mcmr_to_vcstar<El::Complex<double>>(MPI_Comm, int, int, int, int, El::Complex<double>*, El::Complex<double>*, cudaStream_t);
